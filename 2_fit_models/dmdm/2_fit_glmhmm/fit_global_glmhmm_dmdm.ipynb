{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fit GLM-HMM to all DMDM data**\n",
    "---\n",
    "We next fit GLM-HMM to the all the animals in the dataset. This step is important as it searchs thorough the parameter space and find possible parameters needed to fit GLM-HMM on individual animal later. We only use outcomes `y` as a dependent variable to simplify the model here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HPC setting**\n",
    "Ashwood's original script is written in python scirpts. Here, we rewrite it in Jupyter to make it more user-friendly to run on HPC with `dask`. [This](https://github.com/pierreglaser/hpc-tutorial/tree/main) is very useful resource to get familiar with `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/htcondor.py:6: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import parse_bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.234.51:8787/status\n"
     ]
    }
   ],
   "source": [
    "# allocate the computing resources\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from joblib import Memory, Parallel, delayed, parallel_backend\n",
    "from threadpoolctl import threadpool_limits\n",
    "from tqdm import tqdm\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    workers=0,      # create the workers \"lazily\" (upon cluster.scal)\n",
    "    memory='64g',   # amount of RAM per worker\n",
    "    processes=1,    # number of execution units per worker (threads and processes)\n",
    "    cores=1,        # among those execution units, number of processes\n",
    "    walltime=\"48:00:00\",\n",
    "    worker_extra_args=[\"--resources GPU=1\"], # the only way to add GPUs\n",
    "    local_directory='/nfs/nhome/live/skuroda/jobs', # set your path to save log\n",
    "    log_directory='/nfs/nhome/live/skuroda/jobs' # set your path to save log\n",
    ")   \n",
    "\n",
    "memory = Memory('/nfs/nhome/live/skuroda/joblib-cache') # set your path\n",
    "\n",
    "n = 10\n",
    "cluster.scale(n)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit GLM-HMM to all animals**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- load modules -------\n",
    "import autograd.numpy as np\n",
    "import numpy as onp\n",
    "import autograd.numpy.random as npr\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from glm_hmm_utils import fit_glm_hmm\n",
    "sys.path.append('../') # a lazy trick to search parent dir\n",
    "# https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "from data_io import get_file_dir, load_session_fold_lookup, load_data, load_glm_vectors\n",
    "from data_labels import create_abort_mask, partition_data_by_session\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of jobs: 200\n"
     ]
    }
   ],
   "source": [
    "# ------- setup variables -------\n",
    "dname = 'dataAllMice'\n",
    "C = 3  # number of output types/categories\n",
    "D = 1  # data (observations) dimension\n",
    "K_vals = [2, 3, 4, 5]\n",
    "N_initializations = 10 #20\n",
    "num_folds = 5\n",
    "\n",
    "nested_outcome = OrderedDict() # define nested structure for behavioral outcomes\n",
    "nested_outcome[\"Baseline\"] = [2]\n",
    "nested_outcome[\"Change\"] = [0, 1]\n",
    "\n",
    "cluster_job_arr = []\n",
    "for K in K_vals:\n",
    "    for i in range(num_folds):\n",
    "        for j in range(N_initializations):\n",
    "            cluster_job_arr.append([K, i, j])\n",
    "\n",
    "N_em_iters = 300  # number of EM iterations\n",
    "global_fit = True\n",
    "transition_alpha = 1 # perform mle => set transition_alpha to 1\n",
    "prior_sigma = 100\n",
    "\n",
    "print('Total number of jobs: {}'.format(len(cluster_job_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- setup path and load data -------\n",
    "data_dir =  get_file_dir().parents[1] / \"data\" / \"dmdm\" / dname / 'data_for_cluster'\n",
    "# Create directory for results:\n",
    "try: \n",
    "    results_dir = get_file_dir().parents[1] / \"results\" / \"dmdm_global_fit\" / dname\n",
    "except:\n",
    "    raise FileNotFoundError('Run GLM first to initialize parameters')\n",
    "\n",
    "\n",
    "#  read in data and train/test split\n",
    "animal_file = data_dir / 'all_animals_concat.npz'\n",
    "session_fold_lookup_table = load_session_fold_lookup(\n",
    "    data_dir / 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "inpt_y, inpt_rt, y, session, rt, stim_onset = load_data(animal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_GLMHMM_y(inpt_y, y, session, \n",
    "                 session_fold_lookup_table, \n",
    "                 global_fit,\n",
    "                 transition_alpha,\n",
    "                 prior_sigma,\n",
    "                 results_dir,\n",
    "                 params):\n",
    "    \n",
    "    [K, fold, iter] = params\n",
    "\n",
    "    # Append a column of ones to inpt to represent the bias covariate:\n",
    "    inpt_y = np.hstack((inpt_y, np.ones((len(inpt_y),1))))\n",
    "    y = y.astype('int')\n",
    "    # Identify violations for exclusion:\n",
    "    abort_idx = np.where(y == 3)[0]\n",
    "    nonviolation_idx, mask = create_abort_mask(abort_idx, inpt_y.shape[0])\n",
    "\n",
    "    #  GLM weights to use to initialize GLM-HMM\n",
    "    if global_fit == True:\n",
    "        init_param_file = results_dir / 'GLM' / ('fold_' + str(fold)) / 'variables_of_interest_y_iter_0.npz'\n",
    "    else:\n",
    "        raise NotImplementedError('This notebook only runs global fitting')\n",
    "\n",
    "    # Create save directory for this initialization/fold combination:\n",
    "    saving_directory = results_dir / (\"GLM_HMM_y_K_\" + str(K)) / (\"fold_\" + str(fold)) / ('iter_' + str(iter))\n",
    "    saving_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    launch_glm_hmm_job(inpt_y,\n",
    "                       y,\n",
    "                       session,\n",
    "                       mask,\n",
    "                       session_fold_lookup_table,\n",
    "                       K,\n",
    "                       D,\n",
    "                       C,\n",
    "                       N_em_iters,\n",
    "                       transition_alpha,\n",
    "                       prior_sigma,\n",
    "                       fold,\n",
    "                       iter,\n",
    "                       global_fit,\n",
    "                       init_param_file,\n",
    "                       saving_directory)\n",
    "\n",
    "\n",
    "def launch_glm_hmm_job(inpt, y, session, mask, session_fold_lookup_table, K, D,\n",
    "                       C, N_em_iters, transition_alpha, prior_sigma, fold,\n",
    "                       iter, global_fit, init_param_file, save_directory):\n",
    "    sys.stdout.flush()\n",
    "    sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_this_fold = [str(sess) in sessions_to_keep for sess in session]\n",
    "    this_inpt, this_y, this_session, this_mask = inpt[idx_this_fold, :], \\\n",
    "                                                 y[idx_this_fold, :], \\\n",
    "                                                 session[idx_this_fold], \\\n",
    "                                                 mask[idx_this_fold, :]\n",
    "    \n",
    "    # Only do this so that errors are avoided - these y values will not\n",
    "    # actually be used for anything (due to violation mask)\n",
    "    this_y[np.where(this_y == 3), :] = 2\n",
    "    \n",
    "    inputs, datas, masks = partition_data_by_session(\n",
    "        this_inpt, this_y, this_mask, this_session)\n",
    "    # Read in GLM fit if global_fit = True:\n",
    "    if global_fit == True:\n",
    "        _, params_for_initialization = load_glm_vectors(init_param_file)\n",
    "    else:\n",
    "        raise NotImplementedError('This notebook only runs global fitting')\n",
    "    M = this_inpt.shape[1]\n",
    "\n",
    "    npr.seed(iter)\n",
    "    fit_glm_hmm(datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                K,\n",
    "                D,\n",
    "                M,\n",
    "                C,\n",
    "                N_em_iters,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                global_fit,\n",
    "                params_for_initialization,\n",
    "                save_title=save_directory / ('glm_hmm_y_raw_parameters_itr_' + str(iter) + '.npz')\n",
    "                )\n",
    "    \n",
    "fit_GLMHMM_eachparam = partial(fit_GLMHMM_y, inpt_y, y, session, session_fold_lookup_table, \n",
    "                               global_fit, transition_alpha, prior_sigma, results_dir)        \n",
    "fit_GLMHMM_eachparam_cached = memory.cache(fit_GLMHMM_eachparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Client(cluster) as client: # upload local functions to each worker. They cannot read them with sys.append or sys.insert.\n",
    "    client.wait_for_workers(n)\n",
    "    client.upload_file(str(get_file_dir() / 'data_io.py'))\n",
    "    client.upload_file(str(get_file_dir() / 'data_labels.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  5.6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    with parallel_backend('dask', wait_for_workers_timeout=120):\n",
    "        Parallel(verbose=100)(\n",
    "            delayed(fit_GLMHMM_eachparam)(params) for params in cluster_job_arr\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n"
     ]
    }
   ],
   "source": [
    "# Once finished, shut down the cluster and the client.\n",
    "memory.clear(warn=False)\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Posthoc data processing**\n",
    "---\n",
    "\n",
    "We first create a matrix of size num_models x num_folds containing normalized loglikelihood for both train and test splits. Then, save best parameters from global fits to initialize each animal's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfold_cv import get_best_iter\n",
    "from io import load_glmhmm_data, load_cv_arr\n",
    "from data_postprocessing_utils import get_file_name_for_best_model_fold, \\\n",
    "    permute_transition_matrix, calculate_state_permutation\n",
    "from glm_hmm_utils import plot_states\n",
    "import json\n",
    "\n",
    "model = 'GLM_HMM'\n",
    "labels_for_plot_y = ['CSize', 'COnset', 'PrevRewarded?', 'bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_ = get_best_iter(model, C, num_folds, data_dir, \n",
    "                      results_dir, \n",
    "                      outcome_dict=nested_outcome,\n",
    "                      K_vals=K_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_directory = data_dir / \"best_global_params\"\n",
    "saving_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cvbt_folds_model = load_cv_arr(results_dir / \"cvbt_folds_model_{}.npz\".format(model))\n",
    "\n",
    "for K in K_vals:\n",
    "    print(\"K = \" + str(K))\n",
    "    with open(results_dir / \"/best_init_cvbt_dict_{}.json\".format(model), 'r') as f:\n",
    "        best_init_cvbt_dict = json.load(f)\n",
    "\n",
    "    # Get the file name corresponding to the best initialization for\n",
    "    # given K value\n",
    "    raw_file = get_file_name_for_best_model_fold(\n",
    "        cvbt_folds_model, K, results_dir, best_init_cvbt_dict)\n",
    "    hmm_params, lls = load_glmhmm_data(raw_file)\n",
    "\n",
    "    # Calculate permutation\n",
    "    permutation = calculate_state_permutation(hmm_params)\n",
    "    print(permutation)\n",
    "\n",
    "    # Save parameters for initializing individual fits\n",
    "    weight_vectors = hmm_params[2][permutation]\n",
    "    log_transition_matrix = permute_transition_matrix(\n",
    "        hmm_params[1][0], permutation)\n",
    "    init_state_dist = hmm_params[0][0][permutation]\n",
    "    params_for_individual_initialization = [[init_state_dist],\n",
    "                                            [log_transition_matrix],\n",
    "                                            weight_vectors]\n",
    "\n",
    "    cv_file = results_dir / \"cvbt_folds_model.npz\"\n",
    "    cv_file_train = results_dir / \"cvbt_train_folds_model.npz\"\n",
    "    plot_states(weight_vectors,\n",
    "                log_transition_matrix,\n",
    "                cv_file,\n",
    "                cv_file_train,\n",
    "                saving_directory,\n",
    "                K,\n",
    "                labels_for_plot=labels_for_plot_y)\n",
    "\n",
    "    np.savez(saving_directory + 'best_params_K_' + str(K) + '.npz',\n",
    "             params_for_individual_initialization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
