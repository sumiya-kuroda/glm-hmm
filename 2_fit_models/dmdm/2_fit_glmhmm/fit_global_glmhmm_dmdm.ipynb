{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fit GLM-HMM to all DMDM data**\n",
    "---\n",
    "We next fit GLM-HMM to the all the animals in the dataset. This step is important as it searchs thprough the parameter space and find possible parameters needed to fit GLM-HMM on individual animal later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HPC setting**\n",
    "Ashwood's original script is written in python scirpts. Here, we rewrite it in Jupyter to make it more user-friendly to run on HPC with `dask`. [This](https://github.com/pierreglaser/hpc-tutorial/tree/main) is very useful resource to get familiar with `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/htcondor.py:6: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import parse_bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.234.11:8787/status\n"
     ]
    }
   ],
   "source": [
    "# allocate the computing resources\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from joblib import Memory, Parallel, delayed, parallel_backend\n",
    "from threadpoolctl import threadpool_limits\n",
    "from tqdm import tqdm\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    workers=0,      # create the workers \"lazily\" (upon cluster.scal)\n",
    "    memory='64g',   # amount of RAM per worker\n",
    "    processes=1,    # number of execution units per worker (threads and processes)\n",
    "    cores=1,        # among those execution units, number of processes\n",
    "    walltime=\"48:00:00\",\n",
    "    worker_extra_args=[\"--resources GPU=1\"], # the only way to add GPUs\n",
    "    local_directory='/nfs/nhome/live/skuroda/jobs', # set your path to save log\n",
    "    log_directory='/nfs/nhome/live/skuroda/jobs' # set your path to save log\n",
    ")   \n",
    "\n",
    "memory = Memory('/nfs/nhome/live/skuroda/joblib-cache') # set your path\n",
    "\n",
    "n = 10\n",
    "cluster.scale(n)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit GLM-HMM to all animals**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- load modules -------\n",
    "import autograd.numpy as np\n",
    "import numpy as onp\n",
    "import autograd.numpy.random as npr\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from glm_hmm_utils import fit_glm_hmm\n",
    "sys.path.append('../') # a lazy trick to search parent dir\n",
    "# https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "from data_io import get_file_dir, load_session_fold_lookup, load_data, load_glm_vectors\n",
    "from data_labels import create_violation_mask, partition_data_by_session\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- setup variables -------\n",
    "dname = 'dataAllMice'\n",
    "C = 4  # number of output types/categories\n",
    "D = 1  # data (observations) dimension\n",
    "K_vals = [2, 3, 4, 5]\n",
    "N_initializations = 10 #20\n",
    "num_folds = 5\n",
    "npr.seed(65)  # set seed in case of randomization\n",
    "\n",
    "nested_outcome = OrderedDict() # define nested structure for behavioral outcomes\n",
    "nested_outcome[\"Baseline\"] = [2, 3]\n",
    "nested_outcome[\"Change\"] = [0, 1]\n",
    "\n",
    "labels_for_plot = ['CSize', 'COnset', \n",
    "                   'PrevMiss?', 'PrevHit?', 'PrevFA?', 'PrevAbort?',\n",
    "                   'bias']\n",
    "\n",
    "cluster_job_arr = []\n",
    "for K in K_vals:\n",
    "    for i in range(num_folds):\n",
    "        for j in range(N_initializations):\n",
    "            cluster_job_arr.append([K, i, j])\n",
    "\n",
    "N_em_iters = 400  # number of EM iterations\n",
    "global_fit = True\n",
    "transition_alpha = 1 # perform mle => set transition_alpha to 1\n",
    "prior_sigma = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- setup path and load data -------\n",
    "data_dir =  get_file_dir().parents[1] / \"data\" / \"dmdm\" / dname / 'data_for_cluster'\n",
    "# Create directory for results:\n",
    "try: \n",
    "    results_dir = get_file_dir().parents[1] / \"results\" / \"dmdm_global_fit\" / dname\n",
    "except:\n",
    "    raise FileNotFoundError('Run GLM first to initialize parameters')\n",
    "\n",
    "\n",
    "#  read in data and train/test split\n",
    "animal_file = data_dir / 'all_animals_concat.npz'\n",
    "session_fold_lookup_table = load_session_fold_lookup(\n",
    "    data_dir / 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "inpt, y, session, _, _ = load_data(animal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_GLMHMM(inpt, y, session, \n",
    "                session_fold_lookup_table, \n",
    "                global_fit,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                params):\n",
    "    \n",
    "    [K, fold, iter] = params\n",
    "\n",
    "    # Append a column of ones to inpt to represent the bias covariate:\n",
    "    inpt = np.hstack((inpt, np.ones((len(inpt),1))))\n",
    "    y = y.astype('int')\n",
    "    # Identify violations for exclusion:\n",
    "    violation_idx = np.where(y == -1)[0]\n",
    "    nonviolation_idx, mask = create_violation_mask(violation_idx,\n",
    "                                                    inpt.shape[0])\n",
    "\n",
    "    #  GLM weights to use to initialize GLM-HMM\n",
    "    init_param_file = results_dir / 'GLM' / ('fold_' + str(fold)) / 'variables_of_interest_iter_0.npz'\n",
    "\n",
    "    # Create save directory for this initialization/fold combination:\n",
    "    saving_directory = results_dir / (\"GLM_HMM_K_\" + str(K)) / (\"fold_\" + str(fold)) / ('iter_' + str(iter))\n",
    "    saving_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    launch_glm_hmm_job(inpt,\n",
    "                        y,\n",
    "                        session,\n",
    "                        mask,\n",
    "                        session_fold_lookup_table,\n",
    "                        K,\n",
    "                        D,\n",
    "                        C,\n",
    "                        N_em_iters,\n",
    "                        transition_alpha,\n",
    "                        prior_sigma,\n",
    "                        fold,\n",
    "                        iter,\n",
    "                        global_fit,\n",
    "                        init_param_file,\n",
    "                        saving_directory)\n",
    "\n",
    "def launch_glm_hmm_job(inpt, y, session, mask, session_fold_lookup_table, K, D,\n",
    "                       C, N_em_iters, transition_alpha, prior_sigma, fold,\n",
    "                       iter, global_fit, init_param_file, save_directory):\n",
    "    sys.stdout.flush()\n",
    "    sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_this_fold = [str(sess) in sessions_to_keep for sess in session]\n",
    "    this_inpt, this_y, this_session, this_mask = inpt[idx_this_fold, :], \\\n",
    "                                                 y[idx_this_fold, :], \\\n",
    "                                                 session[idx_this_fold], \\\n",
    "                                                 mask[idx_this_fold]\n",
    "    # Only do this so that errors are avoided - these y values will not\n",
    "    # actually be used for anything (due to violation mask)\n",
    "    inputs, datas, masks = partition_data_by_session(\n",
    "        this_inpt, this_y, this_mask, this_session)\n",
    "    masks = None\n",
    "    # Read in GLM fit if global_fit = True:\n",
    "    if global_fit == True:\n",
    "        _, params_for_initialization = load_glm_vectors(init_param_file)\n",
    "    else:\n",
    "        raise NotImplementedError('This notebook only runs global fitting')\n",
    "    M = this_inpt.shape[1]\n",
    "    npr.seed(iter)\n",
    "    fit_glm_hmm(datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                K,\n",
    "                D,\n",
    "                M,\n",
    "                C,\n",
    "                N_em_iters,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                global_fit,\n",
    "                params_for_initialization,\n",
    "                save_title=save_directory / ('glm_hmm_raw_parameters_itr_' + str(iter) + '.npz')\n",
    "                )\n",
    "    \n",
    "fit_GLMHMM_eachparam = partial(fit_GLMHMM, inpt, y, session, session_fold_lookup_table, \n",
    "                               global_fit, transition_alpha, prior_sigma)        \n",
    "fit_GLMHMM_eachparam_cached = memory.cache(fit_GLMHMM_eachparam)\n",
    "\n",
    "with Client(cluster) as client: # upload local functions to each worker. They cannot read them with sys.append or sys.insert.\n",
    "    client.wait_for_workers(n)\n",
    "    client.upload_file(str(get_file_dir() / 'data_io.py'))\n",
    "    client.upload_file(str(get_file_dir() / 'data_labels.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed: 22.6min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 23.6min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 24.4min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 40.6min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed: 42.8min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed: 46.7min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 48.9min\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed: 52.4min\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed: 54.5min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 63.8min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed: 64.0min\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed: 76.1min\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed: 79.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed: 79.6min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 86.0min\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed: 88.1min\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed: 91.3min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 94.0min\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed: 95.0min\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed: 96.4min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed: 100.2min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 106.6min\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed: 111.6min\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed: 118.3min\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed: 121.7min\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed: 132.1min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 133.9min\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed: 143.3min\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed: 143.9min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 145.1min\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed: 145.3min\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed: 162.3min\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed: 167.4min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 171.3min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 172.2min\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed: 189.6min\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed: 192.3min\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed: 193.0min\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed: 194.7min\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed: 195.2min\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed: 198.5min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 200.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Uncaught exception GET /status/ws (172.24.170.111)\n",
      "HTTPServerRequest(protocol='http', host='192.168.234.11:8787', method='GET', uri='/status/ws', version='HTTP/1.1', remote_ip='172.24.170.111')\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/tornado/websocket.py\", line 942, in _accept_connection\n",
      "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
      "  File \"/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/tornado/web.py\", line 3208, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/bokeh/server/views/ws.py\", line 149, in open\n",
      "    raise ProtocolError(\"Token is expired.\")\n",
      "bokeh.protocol.exceptions.ProtocolError: Token is expired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 213.5min\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed: 213.8min\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed: 214.7min\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed: 215.3min\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 217.0min\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed: 224.1min\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 226.8min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 227.3min\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed: 231.2min\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed: 239.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed: 259.1min\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed: 268.2min\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed: 276.7min\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed: 285.7min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 300.1min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 302.6min\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed: 317.8min\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed: 320.1min\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed: 320.4min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed: 336.1min\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed: 339.9min\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed: 340.6min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 356.9min\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed: 364.0min\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed: 369.2min\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed: 386.3min\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed: 390.4min\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed: 392.1min\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed: 394.8min\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed: 407.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 441.5min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 441.6min\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed: 450.6min\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed: 453.8min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 457.5min\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed: 458.1min\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed: 462.7min\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed: 472.1min\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed: 475.6min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 497.5min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 509.8min\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed: 510.6min\n",
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed: 514.3min\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed: 516.6min\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed: 516.9min\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed: 521.4min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed: 527.1min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 556.8min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 573.2min\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed: 580.9min\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed: 595.1min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 596.6min\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed: 607.4min\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed: 616.3min\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed: 633.0min\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed: 637.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 646.3min\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed: 665.7min\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed: 696.0min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed: 700.8min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed: 747.0min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed: 748.9min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed: 752.1min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed: 769.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 779.6min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed: 788.1min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed: 804.3min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 811.3min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 816.0min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed: 833.0min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed: 891.7min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed: 898.6min\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed: 903.6min\n",
      "[Parallel(n_jobs=-1)]: Done 163 tasks      | elapsed: 911.0min\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed: 937.2min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 947.8min\n",
      "[Parallel(n_jobs=-1)]: Done 166 tasks      | elapsed: 956.3min\n",
      "[Parallel(n_jobs=-1)]: Done 167 tasks      | elapsed: 958.6min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 965.7min\n",
      "[Parallel(n_jobs=-1)]: Done 169 tasks      | elapsed: 1014.0min\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed: 1048.1min\n",
      "[Parallel(n_jobs=-1)]: Done 171 tasks      | elapsed: 1054.5min\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed: 1058.7min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 1079.5min\n",
      "[Parallel(n_jobs=-1)]: Done 174 tasks      | elapsed: 1080.6min\n",
      "[Parallel(n_jobs=-1)]: Done 175 tasks      | elapsed: 1081.5min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 1102.4min\n",
      "[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed: 1129.5min\n",
      "[Parallel(n_jobs=-1)]: Done 178 tasks      | elapsed: 1175.7min\n",
      "[Parallel(n_jobs=-1)]: Done 179 tasks      | elapsed: 1203.3min\n",
      "[Parallel(n_jobs=-1)]: Done 180 tasks      | elapsed: 1206.6min\n",
      "[Parallel(n_jobs=-1)]: Done 181 tasks      | elapsed: 1206.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 out of 200 | elapsed: 1233.2min remaining: 107.2min\n",
      "[Parallel(n_jobs=-1)]: Done 187 out of 200 | elapsed: 1328.3min remaining: 92.3min\n",
      "[Parallel(n_jobs=-1)]: Done 190 out of 200 | elapsed: 1360.1min remaining: 71.6min\n",
      "[Parallel(n_jobs=-1)]: Done 193 out of 200 | elapsed: 1403.0min remaining: 50.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/glmhmm/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/glmhmm/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/glmhmm/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/glmhmm/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    with parallel_backend('dask', wait_for_workers_timeout=60):\n",
    "        Parallel(verbose=100)(\n",
    "            delayed(fit_GLMHMM_eachparam_cached)(params) for params in cluster_job_arr\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:321: FutureWarning: ignoring was deprecated in version 2021.06.1 and will be removed in a future release. Please use contextlib.suppress from the standard library instead.\n",
      "  with ignoring(RuntimeError):  # deleting job when job already gone\n"
     ]
    }
   ],
   "source": [
    "# Once finished, shut down the cluster and the client.\n",
    "memory.clear(warn=False)\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Posthoc data processing**\n",
    "---\n",
    "\n",
    "We first create a matrix of size num_models x num_folds containing normalized loglikelihood for both train and test splits. Then, save best parameters from global fits to initialize each animal's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfold_cv import get_best_iter\n",
    "from io import load_glmhmm_data, load_cv_arr\n",
    "from data_postprocessing_utils import get_file_name_for_best_model_fold, \\\n",
    "    permute_transition_matrix, calculate_state_permutation\n",
    "from glm_hmm_utils import plot_states\n",
    "import json\n",
    "\n",
    "model = 'GLM_HMM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_ = get_best_iter(model, C, num_folds, data_dir, \n",
    "                      results_dir, \n",
    "                      outcome_dict=nested_outcome,\n",
    "                      K_vals=K_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_directory = data_dir / \"best_global_params\"\n",
    "saving_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cvbt_folds_model = load_cv_arr(results_dir / \"cvbt_folds_model_{}.npz\".format(model))\n",
    "\n",
    "for K in K_vals:\n",
    "    print(\"K = \" + str(K))\n",
    "    with open(results_dir / \"/best_init_cvbt_dict_{}.json\".format(model), 'r') as f:\n",
    "        best_init_cvbt_dict = json.load(f)\n",
    "\n",
    "    # Get the file name corresponding to the best initialization for\n",
    "    # given K value\n",
    "    raw_file = get_file_name_for_best_model_fold(\n",
    "        cvbt_folds_model, K, results_dir, best_init_cvbt_dict)\n",
    "    hmm_params, lls = load_glmhmm_data(raw_file)\n",
    "\n",
    "    # Calculate permutation\n",
    "    permutation = calculate_state_permutation(hmm_params)\n",
    "    print(permutation)\n",
    "\n",
    "    # Save parameters for initializing individual fits\n",
    "    weight_vectors = hmm_params[2][permutation]\n",
    "    log_transition_matrix = permute_transition_matrix(\n",
    "        hmm_params[1][0], permutation)\n",
    "    init_state_dist = hmm_params[0][0][permutation]\n",
    "    params_for_individual_initialization = [[init_state_dist],\n",
    "                                            [log_transition_matrix],\n",
    "                                            weight_vectors]\n",
    "\n",
    "    cv_file = results_dir / \"cvbt_folds_model.npz\"\n",
    "    cv_file_train = results_dir / \"cvbt_train_folds_model.npz\"\n",
    "    plot_states(weight_vectors,\n",
    "                log_transition_matrix,\n",
    "                cv_file,\n",
    "                cv_file_train,\n",
    "                saving_directory,\n",
    "                K,\n",
    "                labels_for_plot=labels_for_plot)\n",
    "\n",
    "    np.savez(saving_directory + 'best_params_K_' + str(K) + '.npz',\n",
    "             params_for_individual_initialization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
