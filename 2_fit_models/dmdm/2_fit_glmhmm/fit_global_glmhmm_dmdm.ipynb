{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fit GLM-HMM to all DMDM data**\n",
    "---\n",
    "We next fit GLM-HMM to the all the animals in the dataset. This step is important as it searchs thprough the parameter space and find possible parameters needed to fit GLM-HMM on individual animal later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HPC setting**\n",
    "Ashwood's original script is written in python scirpts. Here, we rewrite it in Jupyter to make it more user-friendly to run on HPC with `dask`. [This](https://github.com/pierreglaser/hpc-tutorial/tree/main) is very useful resource to get familiar with `dask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/core.py:17: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import format_bytes, parse_bytes, tmpfile, get_ip_interface\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/dask_jobqueue/htcondor.py:6: FutureWarning: parse_bytes is deprecated and will be removed in a future release. Please use dask.utils.parse_bytes instead.\n",
      "  from distributed.utils import parse_bytes\n",
      "/nfs/nhome/live/skuroda/.conda/envs/glmhmm/lib/python3.7/site-packages/distributed/node.py:161: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35465 instead\n",
      "  f\"Port {expected} is already in use.\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.234.51:35465/status\n"
     ]
    }
   ],
   "source": [
    "# allocate the computing resources\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from joblib import Memory, Parallel, delayed, parallel_backend\n",
    "from threadpoolctl import threadpool_limits\n",
    "from tqdm import tqdm\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    workers=0,      # create the workers \"lazily\" (upon cluster.scal)\n",
    "    memory='32g',   # amount of RAM per worker\n",
    "    processes=1,    # number of execution units per worker (threads and processes)\n",
    "    cores=1,        # among those execution units, number of processes\n",
    "    # A lazy trick to avoid matplotlib crash with parallel plotting\n",
    "    walltime=\"24:00:00\",\n",
    "    worker_extra_args=[\"--resources GPU=2\"], # the only way to add GPUs\n",
    "    local_directory='/nfs/nhome/live/skuroda/jobs', # set your path to save log\n",
    "    log_directory='/nfs/nhome/live/skuroda/jobs' # set your path to save log\n",
    ")   \n",
    "\n",
    "memory = Memory('/nfs/nhome/live/skuroda/joblib-cache') # set your path\n",
    "\n",
    "cluster.scale(10)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit GLM-HMM to all animals**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- load modules -------\n",
    "import autograd.numpy as np\n",
    "import numpy as onp\n",
    "import autograd.numpy.random as npr\n",
    "from glm_hmm_utils import get_file_dir, load_session_fold_lookup, \\\n",
    "     load_data, create_violation_mask, fit_glm_hmm, partition_data_by_session, \\\n",
    "     load_glm_vectors, load_global_params\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- setup variables -------\n",
    "dname = 'dataAllMice'\n",
    "C = 4  # number of output types/categories\n",
    "D = 1  # data (observations) dimension\n",
    "K_vals = [2, 3, 4, 5]\n",
    "N_initializations = 10 #20\n",
    "num_folds = 5\n",
    "npr.seed(65)  # set seed in case of randomization\n",
    "\n",
    "nested_outcome = OrderedDict() # define nested structure for behavioral outcomes\n",
    "nested_outcome[\"Baseline\"] = [2, 3]\n",
    "nested_outcome[\"Change\"] = [0, 1]\n",
    "\n",
    "cluster_job_arr = []\n",
    "for K in K_vals:\n",
    "    for i in range(num_folds):\n",
    "        for j in range(N_initializations):\n",
    "            cluster_job_arr.append([K, i, j])\n",
    "\n",
    "N_em_iters = 300  # number of EM iterations\n",
    "global_fit = True\n",
    "transition_alpha = 1 # perform mle => set transition_alpha to 1\n",
    "prior_sigma = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- setup path and load data -------\n",
    "data_dir =  get_file_dir().parents[2] / \"data\" / \"dmdm\" / dname / 'data_for_cluster'\n",
    "# Create directory for results:\n",
    "try: \n",
    "    results_dir = get_file_dir().parents[2] / \"results\" / \"dmdm_global_fit\" / dname\n",
    "except:\n",
    "    raise FileNotFoundError('Run GLM First to initialize parameters')\n",
    "\n",
    "\n",
    "#  read in data and train/test split\n",
    "animal_file = data_dir / 'all_animals_concat.npz'\n",
    "session_fold_lookup_table = load_session_fold_lookup(\n",
    "    data_dir / 'all_animals_concat_session_fold_lookup.npz')\n",
    "\n",
    "inpt, y, session, _, _ = load_data(animal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_GLM_HMM(inpt, y, session, \n",
    "                session_fold_lookup_table, \n",
    "                global_fit,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                params):\n",
    "    \n",
    "    [K, fold, iter] = params\n",
    "\n",
    "    # Append a column of ones to inpt to represent the bias covariate:\n",
    "    inpt = np.hstack((inpt, np.ones((len(inpt),1))))\n",
    "    y = y.astype('int')\n",
    "    # Identify violations for exclusion:\n",
    "    violation_idx = np.where(y == -1)[0]\n",
    "    nonviolation_idx, mask = create_violation_mask(violation_idx,\n",
    "                                                    inpt.shape[0])\n",
    "\n",
    "    #  GLM weights to use to initialize GLM-HMM\n",
    "    init_param_file = results_dir / 'GLM' / ('fold_' + str(fold)) / 'variables_of_interest_iter_0.npz'\n",
    "\n",
    "    # Create save directory for this initialization/fold combination:\n",
    "    saving_directory = results_dir / (\"GLM_HMM_K_\" + str(K)) / (\"fold_\" + str(fold)) / ('iter_' + str(iter))\n",
    "    saving_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    launch_glm_hmm_job(inpt,\n",
    "                        y,\n",
    "                        session,\n",
    "                        mask,\n",
    "                        session_fold_lookup_table,\n",
    "                        K,\n",
    "                        D,\n",
    "                        C,\n",
    "                        N_em_iters,\n",
    "                        transition_alpha,\n",
    "                        prior_sigma,\n",
    "                        fold,\n",
    "                        iter,\n",
    "                        global_fit,\n",
    "                        init_param_file,\n",
    "                        saving_directory)\n",
    "\n",
    "def launch_glm_hmm_job(inpt, y, session, mask, session_fold_lookup_table, K, D,\n",
    "                       C, N_em_iters, transition_alpha, prior_sigma, fold,\n",
    "                       iter, global_fit, init_param_file, save_directory):\n",
    "    sys.stdout.flush()\n",
    "    sessions_to_keep = session_fold_lookup_table[np.where(\n",
    "        session_fold_lookup_table[:, 1] != fold), 0]\n",
    "    idx_this_fold = [str(sess) in sessions_to_keep for sess in session]\n",
    "    this_inpt, this_y, this_session, this_mask = inpt[idx_this_fold, :], \\\n",
    "                                                 y[idx_this_fold, :], \\\n",
    "                                                 session[idx_this_fold], \\\n",
    "                                                 mask[idx_this_fold]\n",
    "    # Only do this so that errors are avoided - these y values will not\n",
    "    # actually be used for anything (due to violation mask)\n",
    "    inputs, datas, masks = partition_data_by_session(\n",
    "        this_inpt, this_y, this_mask, this_session)\n",
    "    # Read in GLM fit if global_fit = True:\n",
    "    if global_fit == True:\n",
    "        _, params_for_initialization = load_glm_vectors(init_param_file)\n",
    "    else:\n",
    "        params_for_initialization = load_global_params(init_param_file)\n",
    "    M = this_inpt.shape[1]\n",
    "    npr.seed(iter)\n",
    "    fit_glm_hmm(datas,\n",
    "                inputs,\n",
    "                masks,\n",
    "                K,\n",
    "                D,\n",
    "                M,\n",
    "                C,\n",
    "                N_em_iters,\n",
    "                transition_alpha,\n",
    "                prior_sigma,\n",
    "                global_fit,\n",
    "                params_for_initialization,\n",
    "                save_title=save_directory / ('glm_hmm_raw_parameters_itr_' + str(iter) + '.npz')\n",
    "                )\n",
    "    \n",
    "fit_GLM_eachparam = partial(fit_GLM_HMM, inpt, y, session, session_fold_lookup_table, \n",
    "                            global_fit, transition_alpha, prior_sigma)        \n",
    "fit_GLM_eachparam_cached = memory.cache(fit_GLM_eachparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 200/200 [00:00<00:00, 201.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with threadpool_limits(limits=1, user_api='blas'):\n",
    "    with parallel_backend('dask', wait_for_workers_timeout=600):\n",
    "        Parallel(verbose=100)(\n",
    "            delayed(fit_GLM_eachparam_cached)(params) for params in cluster_job_arr\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once finished, shut down the cluster and the client\n",
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Posthoc data processing**\n",
    "---\n",
    "\n",
    "Create a matrix of size num_models x num_folds containing normalized loglikelihood for both train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../') # a lazy trick to search parent dir\n",
    "# https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "from kfold_cv import get_best_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GLM_HMM'\n",
    "_,_,_ = get_best_iter(model, C, num_folds, data_dir, \n",
    "                      results_dir, \n",
    "                      outcome_dict=nested_outcome,\n",
    "                      K_max=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters from IBL global fits (for K = 2 to 5) to initialize\n",
    "# each animal's model\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from post_processing_utils import load_glmhmm_data, load_cv_arr, \\\n",
    "    create_cv_frame_for_plotting, get_file_name_for_best_model_fold, \\\n",
    "    permute_transition_matrix, calculate_state_permutation\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_dir = '../../data/ibl/data_for_cluster/'\n",
    "    results_dir = '../../results/ibl_global_fit/'\n",
    "    save_directory = data_dir + \"best_global_params/\"\n",
    "\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    labels_for_plot = ['stim', 'pc', 'wsls', 'bias']\n",
    "\n",
    "    cv_file = results_dir + \"/cvbt_folds_model.npz\"\n",
    "    cvbt_folds_model = load_cv_arr(cv_file)\n",
    "\n",
    "    for K in range(2, 6):\n",
    "        print(\"K = \" + str(K))\n",
    "        with open(results_dir + \"/best_init_cvbt_dict.json\", 'r') as f:\n",
    "            best_init_cvbt_dict = json.load(f)\n",
    "\n",
    "        # Get the file name corresponding to the best initialization for\n",
    "        # given K value\n",
    "        raw_file = get_file_name_for_best_model_fold(\n",
    "            cvbt_folds_model, K, results_dir, best_init_cvbt_dict)\n",
    "        hmm_params, lls = load_glmhmm_data(raw_file)\n",
    "\n",
    "        # Calculate permutation\n",
    "        permutation = calculate_state_permutation(hmm_params)\n",
    "        print(permutation)\n",
    "\n",
    "        # Save parameters for initializing individual fits\n",
    "        weight_vectors = hmm_params[2][permutation]\n",
    "        log_transition_matrix = permute_transition_matrix(\n",
    "            hmm_params[1][0], permutation)\n",
    "        init_state_dist = hmm_params[0][0][permutation]\n",
    "        params_for_individual_initialization = [[init_state_dist],\n",
    "                                                [log_transition_matrix],\n",
    "                                                weight_vectors]\n",
    "\n",
    "        np.savez(\n",
    "            save_directory + 'best_params_K_' + str(K) + '.npz',\n",
    "            params_for_individual_initialization)\n",
    "\n",
    "        # Plot these too:\n",
    "        cols = [\"#e74c3c\", \"#15b01a\", \"#7e1e9c\", \"#3498db\", \"#f97306\"]\n",
    "        fig = plt.figure(figsize=(4 * 8, 10),\n",
    "                         dpi=80,\n",
    "                         facecolor='w',\n",
    "                         edgecolor='k')\n",
    "        plt.subplots_adjust(left=0.1,\n",
    "                            bottom=0.24,\n",
    "                            right=0.95,\n",
    "                            top=0.7,\n",
    "                            wspace=0.8,\n",
    "                            hspace=0.5)\n",
    "        plt.subplot(1, 3, 1)\n",
    "        M = weight_vectors.shape[2] - 1\n",
    "        for k in range(K):\n",
    "            plt.plot(range(M + 1),\n",
    "                     -weight_vectors[k][0],\n",
    "                     marker='o',\n",
    "                     label='State ' + str(k + 1),\n",
    "                     color=cols[k],\n",
    "                     lw=4)\n",
    "        plt.xticks(list(range(0, len(labels_for_plot))),\n",
    "                   labels_for_plot,\n",
    "                   rotation='20',\n",
    "                   fontsize=24)\n",
    "        plt.yticks(fontsize=30)\n",
    "        plt.legend(fontsize=30)\n",
    "        plt.axhline(y=0, color=\"k\", alpha=0.5, ls=\"--\")\n",
    "        # plt.ylim((-3, 14))\n",
    "        plt.ylabel(\"Weight\", fontsize=30)\n",
    "        plt.xlabel(\"Covariate\", fontsize=30, labelpad=20)\n",
    "        plt.title(\"GLM Weights: Choice = R\", fontsize=40)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        transition_matrix = np.exp(log_transition_matrix)\n",
    "        plt.imshow(transition_matrix, vmin=0, vmax=1)\n",
    "        for i in range(transition_matrix.shape[0]):\n",
    "            for j in range(transition_matrix.shape[1]):\n",
    "                text = plt.text(j,\n",
    "                                i,\n",
    "                                np.around(transition_matrix[i, j],\n",
    "                                          decimals=3),\n",
    "                                ha=\"center\",\n",
    "                                va=\"center\",\n",
    "                                color=\"k\",\n",
    "                                fontsize=30)\n",
    "        plt.ylabel(\"Previous State\", fontsize=30)\n",
    "        plt.xlabel(\"Next State\", fontsize=30)\n",
    "        plt.xlim(-0.5, K - 0.5)\n",
    "        plt.ylim(-0.5, K - 0.5)\n",
    "        plt.xticks(range(0, K), ('1', '2', '3', '4', '4', '5', '6', '7',\n",
    "                                 '8', '9', '10')[:K],\n",
    "                   fontsize=30)\n",
    "        plt.yticks(range(0, K), ('1', '2', '3', '4', '4', '5', '6', '7',\n",
    "                                 '8', '9', '10')[:K],\n",
    "                   fontsize=30)\n",
    "        plt.title(\"Retrieved\", fontsize=40)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        cols = [\n",
    "            \"#7e1e9c\", \"#0343df\", \"#15b01a\", \"#bf77f6\", \"#95d0fc\",\n",
    "            \"#96f97b\"\n",
    "        ]\n",
    "        cv_file = results_dir + \"/cvbt_folds_model.npz\"\n",
    "        data_for_plotting_df, loc_best, best_val, glm_lapse_model = \\\n",
    "            create_cv_frame_for_plotting(\n",
    "            cv_file)\n",
    "        cv_file_train = results_dir + \"/cvbt_train_folds_model.npz\"\n",
    "        train_data_for_plotting_df, train_loc_best, train_best_val, \\\n",
    "        train_glm_lapse_model = create_cv_frame_for_plotting(\n",
    "            cv_file_train)\n",
    "\n",
    "        glm_lapse_model_cvbt_means = np.mean(glm_lapse_model, axis=1)\n",
    "        train_glm_lapse_model_cvbt_means = np.mean(train_glm_lapse_model,\n",
    "                                                   axis=1)\n",
    "        g = sns.lineplot(\n",
    "            data_for_plotting_df['model'],\n",
    "            data_for_plotting_df['cv_bit_trial'],\n",
    "            err_style=\"bars\",\n",
    "            mew=0,\n",
    "            color=cols[0],\n",
    "            marker='o',\n",
    "            ci=68,\n",
    "            label=\"test\",\n",
    "            alpha=1,\n",
    "            lw=4)\n",
    "        sns.lineplot(\n",
    "            train_data_for_plotting_df['model'],\n",
    "            train_data_for_plotting_df['cv_bit_trial'],\n",
    "            err_style=\"bars\",\n",
    "            mew=0,\n",
    "            color=cols[1],\n",
    "            marker='o',\n",
    "            ci=68,\n",
    "            label=\"train\",\n",
    "            alpha=1,\n",
    "            lw=4)\n",
    "        plt.xlabel(\"Model\", fontsize=30)\n",
    "        plt.ylabel(\"Normalized LL\", fontsize=30)\n",
    "        plt.xticks([0, 1, 2, 3, 4],\n",
    "                   ['1 State', '2 State', '3 State', '4 State', '5 State'],\n",
    "                   rotation=45,\n",
    "                   fontsize=24)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.axhline(y=glm_lapse_model_cvbt_means[2],\n",
    "                    color=cols[2],\n",
    "                    label=\"Lapse (test)\",\n",
    "                    alpha=0.9,\n",
    "                    lw=4)\n",
    "        plt.legend(loc='upper right', fontsize=30)\n",
    "        plt.tick_params(axis='y')\n",
    "        plt.yticks([0.2, 0.3, 0.4, 0.5], fontsize=30)\n",
    "        plt.ylim((0.2, 0.55))\n",
    "        plt.title(\"Model Comparison\", fontsize=40)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        fig.savefig(results_dir + 'best_params_cross_validation_K_' +\n",
    "                    str(K) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
